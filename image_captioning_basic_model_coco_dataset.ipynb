{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from PIL import Image\n",
    "from cache import cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tf.keras.models import Model  # This does not work!\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding\n",
    "from tensorflow.python.keras.applications import VGG16\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco.set_data_dir(\"/media/ruler/A44C-97D3/data/coco/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://images.cocodataset.org/zips/train2017.zip\n",
      "- Download progress: 22.2%"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 27] File too large",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    280\u001b[0m                 \u001b[0mread\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m                 \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m                 \u001b[0mblocknum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 27] File too large",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6b3dda9fbc52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_download_and_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Downloads/rnd/week5_week6_week7/coco.py\u001b[0m in \u001b[0;36mmaybe_download_and_extract\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mdownload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_download_and_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/rnd/week5_week6_week7/download.py\u001b[0m in \u001b[0;36mmaybe_download_and_extract\u001b[0;34m(url, download_dir)\u001b[0m\n\u001b[1;32m    111\u001b[0m         file_path, _ = urllib.request.urlretrieve(url=url,\n\u001b[1;32m    112\u001b[0m                                                   \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                                                   reporthook=_print_download_progress)\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpuenv/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    282\u001b[0m                 \u001b[0mblocknum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m                     \u001b[0mreporthook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocknum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mread\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 27] File too large"
     ]
    }
   ],
   "source": [
    "coco.maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, filenames_train, captions_train = coco.load_records(train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_train = len(filenames_train)\n",
    "num_images_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, filenames_val, captions_val = coco.load_records(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, size=None):\n",
    "    \"\"\"\n",
    "    Load the image from the given file-path and resize it\n",
    "    to the given size if not None.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the image using PIL.\n",
    "    img = Image.open(path)\n",
    "\n",
    "    # Resize image if desired.\n",
    "    if not size is None:\n",
    "        img = img.resize(size=size, resample=Image.LANCZOS)\n",
    "\n",
    "    # Convert image to numpy array.\n",
    "    img = np.array(img)\n",
    "\n",
    "    # Scale image-pixels so they fall between 0.0 and 1.0\n",
    "    img = img / 255.0\n",
    "\n",
    "    # Convert 2-dim gray-scale array to 3-dim RGB array.\n",
    "    if (len(img.shape) == 2):\n",
    "        img = np.repeat(img[:, :, np.newaxis], 3, axis=2)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(idx, train):\n",
    "    \"\"\"\n",
    "    Load and plot an image from the training- or validation-set\n",
    "    with the given index.\n",
    "    \"\"\"\n",
    "\n",
    "    if train:\n",
    "        # Use an image from the training-set.\n",
    "        dir = coco.train_dir\n",
    "        filename = filenames_train[idx]\n",
    "        captions = captions_train[idx]\n",
    "    else:\n",
    "        # Use an image from the validation-set.\n",
    "        dir = coco.val_dir\n",
    "        filename = filenames_val[idx]\n",
    "        captions = captions_val[idx]\n",
    "\n",
    "    # Path for the image-file.\n",
    "    path = os.path.join(dir, filename)\n",
    "\n",
    "    # Print the captions for this image.\n",
    "    for caption in captions:\n",
    "        print(caption)\n",
    "    \n",
    "    # Load the image and plot it.\n",
    "    img = load_image(path)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(idx=1, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = VGG16(include_top=True, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_layer = image_model.get_layer('fc2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model_transfer = Model(inputs=image_model.input,\n",
    "                             outputs=transfer_layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = K.int_shape(image_model.input)[1:3]\n",
    "img_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_values_size = K.int_shape(transfer_layer.output)[1]\n",
    "transfer_values_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress(count, max_count):\n",
    "    # Percentage completion.\n",
    "    pct_complete = count / max_count\n",
    "\n",
    "    # Status-message. Note the \\r which means the line should\n",
    "    # overwrite itself.\n",
    "    msg = \"\\r- Progress: {0:.1%}\".format(pct_complete)\n",
    "\n",
    "    # Print it.\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(data_dir, filenames, batch_size=32):\n",
    "    \"\"\"\n",
    "    Process all the given files in the given data_dir using the\n",
    "    pre-trained image-model and return their transfer-values.\n",
    "    \n",
    "    Note that we process the images in batches to save\n",
    "    memory and improve efficiency on the GPU.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Number of images to process.\n",
    "    num_images = len(filenames)\n",
    "\n",
    "    # Pre-allocate input-batch-array for images.\n",
    "    shape = (batch_size,) + img_size + (3,)\n",
    "    image_batch = np.zeros(shape=shape, dtype=np.float16)\n",
    "\n",
    "    # Pre-allocate output-array for transfer-values.\n",
    "    # Note that we use 16-bit floating-points to save memory.\n",
    "    shape = (num_images, transfer_values_size)\n",
    "    transfer_values = np.zeros(shape=shape, dtype=np.float16)\n",
    "\n",
    "    # Initialize index into the filenames.\n",
    "    start_index = 0\n",
    "\n",
    "    # Process batches of image-files.\n",
    "    while start_index < num_images:\n",
    "        # Print the percentage-progress.\n",
    "        print_progress(count=start_index, max_count=num_images)\n",
    "\n",
    "        # End-index for this batch.\n",
    "        end_index = start_index + batch_size\n",
    "\n",
    "        # Ensure end-index is within bounds.\n",
    "        if end_index > num_images:\n",
    "            end_index = num_images\n",
    "\n",
    "        # The last batch may have a different batch-size.\n",
    "        current_batch_size = end_index - start_index\n",
    "\n",
    "        # Load all the images in the batch.\n",
    "        for i, filename in enumerate(filenames[start_index:end_index]):\n",
    "            # Path for the image-file.\n",
    "            path = os.path.join(data_dir, filename)\n",
    "\n",
    "            # Load and resize the image.\n",
    "            # This returns the image as a numpy-array.\n",
    "            img = load_image(path, size=img_size)\n",
    "\n",
    "            # Save the image for later use.\n",
    "            image_batch[i] = img\n",
    "\n",
    "        # Use the pre-trained image-model to process the image.\n",
    "        # Note that the last batch may have a different size,\n",
    "        # so we only use the relevant images.\n",
    "        transfer_values_batch = \\\n",
    "            image_model_transfer.predict(image_batch[0:current_batch_size])\n",
    "\n",
    "        # Save the transfer-values in the pre-allocated array.\n",
    "        transfer_values[start_index:end_index] = \\\n",
    "            transfer_values_batch[0:current_batch_size]\n",
    "\n",
    "        # Increase the index for the next loop-iteration.\n",
    "        start_index = end_index\n",
    "\n",
    "    # Print newline.\n",
    "    print()\n",
    "\n",
    "    return transfer_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_train():\n",
    "    print(\"Processing {0} images in training-set ...\".format(len(filenames_train)))\n",
    "\n",
    "    # Path for the cache-file.\n",
    "    cache_path = os.path.join(coco.data_dir,\n",
    "                              \"transfer_values_train.pkl\")\n",
    "\n",
    "    # If the cache-file already exists then reload it,\n",
    "    # otherwise process all images and save their transfer-values\n",
    "    # to the cache-file so it can be reloaded quickly.\n",
    "    transfer_values = cache(cache_path=cache_path,\n",
    "                            fn=process_images,\n",
    "                            data_dir=coco.train_dir,\n",
    "                            filenames=filenames_train)\n",
    "\n",
    "    return transfer_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_val():\n",
    "    print(\"Processing {0} images in validation-set ...\".format(len(filenames_val)))\n",
    "\n",
    "    # Path for the cache-file.\n",
    "    cache_path = os.path.join(coco.data_dir, \"transfer_values_val.pkl\")\n",
    "\n",
    "    # If the cache-file already exists then reload it,\n",
    "    # otherwise process all images and save their transfer-values\n",
    "    # to the cache-file so it can be reloaded quickly.\n",
    "    transfer_values = cache(cache_path=cache_path,\n",
    "                            fn=process_images,\n",
    "                            data_dir=coco.val_dir,\n",
    "                            filenames=filenames_val)\n",
    "\n",
    "    return transfer_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "transfer_values_train = process_images_train()\n",
    "print(\"dtype:\", transfer_values_train.dtype)\n",
    "print(\"shape:\", transfer_values_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "transfer_values_val = process_images_val()\n",
    "print(\"dtype:\", transfer_values_val.dtype)\n",
    "print(\"shape:\", transfer_values_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_start = 'ssss '\n",
    "mark_end = ' eeee'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_captions(captions_listlist):\n",
    "    captions_marked = [[mark_start + caption + mark_end\n",
    "                        for caption in captions_list]\n",
    "                        for captions_list in captions_listlist]\n",
    "    \n",
    "    return captions_marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_train_marked = mark_captions(captions_train)\n",
    "captions_train_marked[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(captions_listlist):\n",
    "    captions_list = [caption\n",
    "                     for captions_list in captions_listlist\n",
    "                     for caption in captions_list]\n",
    "    \n",
    "    return captions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_train_flat = flatten(captions_train_marked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrap(Tokenizer):\n",
    "    \"\"\"Wrap the Tokenizer-class from Keras with more functionality.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, num_words=None):\n",
    "        \"\"\"\n",
    "        :param texts: List of strings with the data-set.\n",
    "        :param num_words: Max number of words to use.\n",
    "        \"\"\"\n",
    "\n",
    "        Tokenizer.__init__(self, num_words=num_words)\n",
    "\n",
    "        # Create the vocabulary from the texts.\n",
    "        self.fit_on_texts(texts)\n",
    "\n",
    "        # Create inverse lookup from integer-tokens to words.\n",
    "        self.index_to_word = dict(zip(self.word_index.values(),\n",
    "                                      self.word_index.keys()))\n",
    "\n",
    "    def token_to_word(self, token):\n",
    "        \"\"\"Lookup a single word from an integer-token.\"\"\"\n",
    "\n",
    "        word = \" \" if token == 0 else self.index_to_word[token]\n",
    "        return word \n",
    "\n",
    "    def tokens_to_string(self, tokens):\n",
    "        \"\"\"Convert a list of integer-tokens to a string.\"\"\"\n",
    "\n",
    "        # Create a list of the individual words.\n",
    "        words = [self.index_to_word[token]\n",
    "                 for token in tokens\n",
    "                 if token != 0]\n",
    "        \n",
    "        # Concatenate the words to a single string\n",
    "        # with space between all the words.\n",
    "        text = \" \".join(words)\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def captions_to_tokens(self, captions_listlist):\n",
    "        \"\"\"\n",
    "        Convert a list-of-list with text-captions to\n",
    "        a list-of-list of integer-tokens.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Note that text_to_sequences() takes a list of texts.\n",
    "        tokens = [self.texts_to_sequences(captions_list)\n",
    "                  for captions_list in captions_listlist]\n",
    "        \n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenizer = TokenizerWrap(texts=captions_train_flat,\n",
    "                          num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_start = tokenizer.word_index[mark_start.strip()]\n",
    "token_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_end = tokenizer.word_index[mark_end.strip()]\n",
    "token_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokens_train = tokenizer.captions_to_tokens(captions_train_marked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_train_marked[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
